{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane' 'automobile' 'bird' 'cat' 'deer' 'dog' 'frog' 'horse' 'ship'\n",
      " 'truck']\n",
      "(50000, 32, 32, 3) (50000,)\n",
      "(10000, 32, 32, 3) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from cifar_data import CIFAR10\n",
    "\n",
    "train_data = CIFAR10(is_train=True)\n",
    "test_data  = CIFAR10(is_train=False, shuffle=False)\n",
    "\n",
    "print(train_data.class_names)\n",
    "print(train_data.X.shape, train_data.y.shape)\n",
    "print(test_data.X.shape, test_data.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 샘플 출력 및 시각화\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_idx = np.random.randint(0, len(train_data.X), 32)\n",
    "\n",
    "plot_images = train_data.X[plot_idx]\n",
    "plot_labels = train_data.y[plot_idx]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "for i, (img, label) in enumerate(zip(plot_images, plot_labels)):\n",
    "    a = fig.add_subplot(4, 8, i+1)\n",
    "    a.axis(\"off\")\n",
    "    a.set_title(train_data.class_names[label])\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm_params(is_training):\n",
    "    return {\n",
    "        \"decay\": 0.9,\n",
    "        \"updates_collections\": None,\n",
    "        \"is_training\": is_training\n",
    "    }\n",
    "\n",
    "def arg_scope(is_training):\n",
    "    with slim.arg_scope([slim.conv2d],\n",
    "        weights_initializer=slim.variance_scaling_initializer(),\n",
    "        activation_fn=tf.nn.relu,\n",
    "        normalizer_fn=slim.batch_norm,\n",
    "        normalizer_params=batch_norm_params(is_training),\n",
    "        stride=1, padding=\"SAME\"):\n",
    "        with slim.arg_scope([slim.batch_norm],\n",
    "                            **batch_norm_params(is_training)) as arg_scp:\n",
    "            return arg_scp\n",
    "        \n",
    "def cifar_net(inputs, scope=\"cifarnet\"):\n",
    "    net = inputs\n",
    "    \n",
    "    with tf.variable_scope(scope) as scp:\n",
    "        end_pts_collection = scp.name+\"_end_pts\" # cifarnet_end_pts\n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                            outputs_collections=end_pts_collection):\n",
    "            net = slim.conv2d(net, 64, [3, 3], scope=\"conv1\") # [32, 32, 64]\n",
    "            net = slim.max_pool2d(net, [2, 2], scope=\"pool1\") # [16, 16, 64]\n",
    "            \n",
    "            net = slim.conv2d(net, 128, [3, 3], scope=\"conv2\") # [16, 16, 128]\n",
    "            net = slim.max_pool2d(net, [2, 2], scope=\"pool2\")  # [8, 8, 128]\n",
    "            \n",
    "            net = slim.conv2d(net, 128, [3, 3], scope=\"conv3\") # [8, 8, 128]\n",
    "            net = slim.max_pool2d(net, [2, 2], scope=\"pool3\")  # [4, 4, 128]\n",
    "            \n",
    "            net = slim.conv2d(net, 256, [3, 3], scope=\"conv4\") # [4, 4, 256]\n",
    "                        \n",
    "            # FC 레이어를 conv2d 레이어로 구현 가능\n",
    "            net = slim.conv2d(net, 512, [4, 4], \n",
    "                              padding=\"VALID\", scope=\"fc1\")  # [1, 1, 512]\n",
    "            net = slim.conv2d(net, 512, [1, 1], scope=\"fc2\") # [1, 1, 512]\n",
    "            net = slim.conv2d(net, 10, [1, 1],               # [1, 1, 10]\n",
    "                              activation_fn=None,\n",
    "                              normalizer_fn=None,\n",
    "                              normalizer_params=None,\n",
    "                              scope=\"logit\")\n",
    "            \n",
    "            net = tf.squeeze(net, [1, 2], name=\"logit/squeeze\")\n",
    "\n",
    "            end_pts = slim.utils.convert_collection_to_dict(end_pts_collection)  \n",
    "            end_pts[\"prediction\"] = slim.softmax(net, scope=\"prediction\")\n",
    "    return net, end_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifarnet/conv1 (?, 32, 32, 64)\n",
      "cifarnet/conv2 (?, 16, 16, 128)\n",
      "cifarnet/conv3 (?, 8, 8, 128)\n",
      "cifarnet/conv4 (?, 4, 4, 256)\n",
      "cifarnet/fc1 (?, 1, 1, 512)\n",
      "cifarnet/fc2 (?, 1, 1, 512)\n",
      "cifarnet/logit (?, 1, 1, 10)\n",
      "prediction (?, 10)\n",
      "(?, 10)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "labels = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool) # 배치 정규화 및 dropout 사용시 필요\n",
    "\n",
    "with slim.arg_scope(arg_scope(is_training)) as scp:\n",
    "    logit, end_pts = cifar_net(inputs)\n",
    "    \n",
    "for k, v in end_pts.items():\n",
    "    print(k, v.get_shape())\n",
    "print(logit.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logit))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_op)\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "sess = tf.Session(config=sess_config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(10000):\n",
    "    batch_X, batch_y = train_data.next_batch(128)\n",
    "    _, loss = sess.run([opt, loss_op], feed_dict={inputs: batch_X, labels:batch_y, is_training: True})\n",
    "    \n",
    "    if (step+1) % 500 == 0:\n",
    "        print(step+1, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 테스트 시 사용하는 연산 그래프 정의\n",
    "pred_op = end_pts[\"prediction\"]\n",
    "correct_op = tf.equal(labels, tf.argmax(pred_op, 1))\n",
    "num_correct_op = tf.reduce_sum(tf.cast(correct_op, tf.float32))\n",
    "\n",
    "num_correct = 0\n",
    "while True:\n",
    "    batch_X, batch_y = test_data.next_batch(128)\n",
    "    num_correct += sess.run(num_correct_op, \n",
    "        feed_dict={inputs:batch_X, labels:batch_y, is_training: False})\n",
    "    \n",
    "    if test_data.epoch_done:\n",
    "        break\n",
    "\n",
    "acc_test = num_correct / len(test_data.X)\n",
    "print(\"step\", step+1, \"test_accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
