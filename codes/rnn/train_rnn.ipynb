{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import tensorflow.contrib.layers as layers\n",
    "from text_loader import TextLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_layers  = 3\n",
    "hidden_size = 512\n",
    "batch_size  = 200\n",
    "max_length  = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "loader = TextLoader(\"data/hamlet.txt\")\n",
    "vocab_size = len(loader.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, [None, max_length])\n",
    "y = tf.placeholder(tf.int32, [None, max_length]) # [N, seqlne]\n",
    "\n",
    "x_one_hot = tf.one_hot(X, vocab_size)\n",
    "y_one_hot = tf.one_hot(y, vocab_size) # [N, seqlen, vocab_size]\n",
    "\n",
    "cells = [rnn.BasicLSTMCell(hidden_size) for _ in range(num_layers)]\n",
    "cells = rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "outputs, _ = tf.nn.dynamic_rnn(cells, x_one_hot, \n",
    "                                    initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "outputs = tf.reshape(outputs, [-1, hidden_size])    # [N x seqlen, hidden]\n",
    "logits = layers.fully_connected(outputs, vocab_size,\n",
    "                                activation_fn=None) # [N x seqlen, vocab_size]\n",
    "y_flat = tf.reshape(y_one_hot, [-1, vocab_size])    # [N x seqlen, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_op = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_flat) # [N x seqlen]\n",
    "loss_op = tf.reduce_mean(loss_op)\n",
    "\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_op)\n",
    "\n",
    "y_softmax = tf.nn.softmax(logits)         # [N x seqlen, vocab_size]\n",
    "pred = tf.argmax(y_softmax, axis=1)       # [N x seqlen]\n",
    "pred = tf.reshape(pred, [batch_size, -1]) # [N, seqlen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000500 step, loss:1.8795\n",
      "t totr ttet tialnee  tot te t\n",
      "\n",
      "00001000 step, loss:1.3221\n",
      "heatun aonntut tlap to that It\n",
      "00001500 step, loss:1.0667\n",
      "\tAMLET\tWi y do ntill, my the e\n",
      "00002000 step, loss:0.6390\n",
      "tes vophew's purpose, -to supp\n",
      "00002500 step, loss:0.5246\n",
      "ur af an. I'll have these play\n",
      "00003000 step, loss:0.5171\n",
      "ut iartted word:\n",
      "\tO heavy burt\n",
      "00003500 step, loss:0.4784\n",
      "lee a gpar i' the dirkest nigh\n",
      "00004000 step, loss:0.4209\n",
      "ueoou, shese are the stops.\n",
      "\n",
      "G\n",
      "00004500 step, loss:0.3246\n",
      " G CLAUDIUS\tAhanks, Rosencrant\n",
      "00005000 step, loss:0.3956\n",
      "tLAUDIUS\tW  salls right.\n",
      "\tYou \n",
      "00005500 step, loss:0.3701\n",
      " r ti hur watch up; and by my \n",
      "00006000 step, loss:0.3716\n",
      "u be  since I am still possess\n",
      "00006500 step, loss:0.3909\n",
      "  tey he walk\n",
      "\tThan may be giv\n",
      "00007000 step, loss:0.3962\n",
      "taart \n",
      "\n",
      "KING CLAUDIUS\tThyself \n",
      "00007500 step, loss:0.3946\n",
      "tet tr not to mine uncle's bed\n",
      "00008000 step, loss:0.3965\n",
      " ahlc  well, well.\n",
      "\n",
      "OPHELIA\tMy\n",
      "00008500 step, loss:0.3322\n",
      "  ntuJulius Caesar: I was kill\n",
      "00009000 step, loss:0.3328\n",
      "e wuarter'd, hath but one part\n",
      "00009500 step, loss:0.3632\n",
      "  uction.in's aspect,\n",
      "\tA broke\n",
      "00010000 step, loss:0.3507\n",
      "e  tror in thanks; but I\n",
      "\tthan\n",
      "00010500 step, loss:0.3208\n",
      "s the mews?\n",
      "\n",
      "ROSENCRANTZ\tNone,\n",
      "00011000 step, loss:0.3802\n",
      "  rsbpon your own liberty, if\n",
      "\n",
      "00011500 step, loss:0.2947\n",
      "tnd wxr. O my dear Gertrude, t\n",
      "00012000 step, loss:0.3524\n",
      "tevt to-morrow night. You coul\n",
      "00012500 step, loss:0.3661\n",
      "uno is that?\n",
      "\n",
      "\t[Re-enter OPHEL\n",
      "00013000 step, loss:0.3084\n",
      "tared\n",
      "\tDoubt truth to be a lia\n",
      "00013500 step, loss:0.3618\n",
      "teais dad, Laertes.\n",
      "\n",
      "QUEEN GER\n",
      "00014000 step, loss:0.3508\n",
      " th save shuffled off this mor\n",
      "00014500 step, loss:0.3518\n",
      "tyct br England; you know that\n",
      "00015000 step, loss:0.3381\n",
      "es scm, lies where it falls,\n",
      "\t\n",
      "00015500 step, loss:0.3397\n",
      " trrsion, lets go by\n",
      "\tThe impo\n",
      "00016000 step, loss:0.3031\n",
      "utive I a noble father lost;\n",
      "\t\n",
      "00016500 step, loss:0.3055\n",
      "L III\n",
      "\n",
      "\n",
      "\n",
      "SCENE I\t\t\tA room in t\n",
      "00017000 step, loss:0.3072\n",
      "\tHaptain\tA will do't, my lord.\n",
      "00017500 step, loss:0.3162\n",
      "tfr sourin Hamlet?\n",
      "\n",
      "HAMLET\tExc\n",
      "00018000 step, loss:0.3614\n",
      " n  bave heard, how I am punis\n",
      "00018500 step, loss:0.3265\n",
      "eoat \n",
      "\tPurpose is but the slav\n",
      "00019000 step, loss:0.3493\n",
      "n d\tTequite him for your fathe\n",
      "00019500 step, loss:0.3302\n",
      " an ye do not, it's no great m\n",
      "00020000 step, loss:0.3336\n",
      "\tCENE II\tThe platform.\n",
      "\n",
      "\n",
      "\t[Ent\n",
      "00020500 step, loss:0.3370\n",
      " and todst persuade revenge,\n",
      "\t\n",
      "00021000 step, loss:0.3053\n",
      "he esost a father;\n",
      "\tThat fathe\n",
      "00021500 step, loss:0.3295\n",
      "  t ll mark the play.\n",
      "\n",
      "Prologu\n",
      "00022000 step, loss:0.3391\n",
      "ty leus of nature\n",
      "\tAre burnt a\n",
      "00022500 step, loss:0.3252\n",
      " ,\n",
      "\n",
      "HAMLET\tW                 I\n",
      "00023000 step, loss:0.3430\n",
      " tot tll sleep? while, to my s\n",
      "00023500 step, loss:0.3397\n",
      "r herods Herod: pray you, avoi\n",
      "00024000 step, loss:0.3493\n",
      " n iaurlish priest,\n",
      "\tA ministe\n",
      "00024500 step, loss:0.3322\n",
      " nt ROSENCRANTZ and GUILDENSTE\n",
      "00025000 step, loss:0.3234\n",
      "  aome \n",
      "\n",
      "\t[Enter HORATIO and M\n",
      "00025500 step, loss:0.3338\n",
      "LAUDIUS\tOaertes, y must commun\n",
      "00026000 step, loss:0.3384\n",
      "oALDO\tA                 My lor\n",
      "00026500 step, loss:0.3372\n",
      "  \tAhyse worth, if praises may\n",
      "00027000 step, loss:0.3548\n",
      "eoak yhee off; look, where it \n",
      "00027500 step, loss:0.3486\n",
      "t wmmoward?\n",
      "\tWho calls me vill\n",
      "00028000 step, loss:0.2708\n",
      "r tf doors he went without the\n",
      "00028500 step, loss:0.3529\n",
      "e seysconfined to fast in fire\n",
      "00029000 step, loss:0.3131\n",
      "\tTn iovel as the cannon to his\n",
      "00029500 step, loss:0.3627\n",
      "nld tompact,\n",
      "\tWell ratified by\n",
      "00030000 step, loss:0.3082\n",
      " tokuid dew of youth\n",
      "\tContagio\n",
      "00030500 step, loss:0.3227\n",
      "tf tur nwn:\n",
      "\tSo think thou wil\n",
      "00031000 step, loss:0.3516\n",
      "lrnnce\n",
      "\tos therein are set dow\n",
      "00031500 step, loss:0.2855\n",
      "  g I'll catch the conscience \n",
      "00032000 step, loss:0.3759\n",
      "e wnd tother to Hamlet.\n",
      "\t(QUEE\n",
      "00032500 step, loss:0.3254\n",
      "nt ao hchool in Wittenberg,\n",
      "\tI\n",
      "00033000 step, loss:0.2807\n",
      "lrd, ihat I should think.\n",
      "\n",
      "LOR\n",
      "00033500 step, loss:0.3243\n",
      "Th hhll mhe secrets of my pris\n",
      "00034000 step, loss:0.3324\n",
      "n : I would give you\n",
      "\tsome vio\n",
      "00034500 step, loss:0.3408\n",
      "h tnow y man well, were to\n",
      "\tkn\n",
      "00035000 step, loss:0.3191\n",
      "ud,\n",
      "\n",
      "HAMLET\tWow say you, then;\n",
      "00035500 step, loss:0.2898\n",
      "torg a down a-down,\n",
      "\tAn you ca\n",
      "00036000 step, loss:0.2724\n",
      "et not been\n",
      "\ta gentlewoman, sh\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, \"checkpoints/char-rnn\")\n",
    "    for step in range(100000):\n",
    "        batch_X, batch_y = loader.next_batch(batch_size, max_length)\n",
    "        loss, _ = sess.run([loss_op, opt], feed_dict={X: batch_X, y: batch_y})\n",
    "        \n",
    "        if (step+1) % 500 == 0:\n",
    "            print(\"{:08d} step, loss:{:.4f}\".format(step+1, loss))\n",
    "            \n",
    "            random = np.random.randint(0, batch_size)\n",
    "            results = sess.run(pred, feed_dict={X: batch_X})\n",
    "            words = [loader.words[word] for word in results[random]]\n",
    "            print(\"\".join(words))\n",
    "     \n",
    "        if (step+1) % 5000 == 0: \n",
    "            saver.save(sess, \"checkpoints/char-rnn_\"+str(step+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
