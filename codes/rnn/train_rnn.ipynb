{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import tensorflow.contrib.layers as layers\n",
    "from text_loader import TextLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_layers  = 3\n",
    "hidden_size = 512\n",
    "batch_size  = 200\n",
    "max_length  = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "loader = TextLoader(\"data/hamlet.txt\")\n",
    "vocab_size = len(loader.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, [None, max_length])\n",
    "y = tf.placeholder(tf.int32, [None, max_length]) # [N, seqlne]\n",
    "\n",
    "x_one_hot = tf.one_hot(X, vocab_size)\n",
    "y_one_hot = tf.one_hot(y, vocab_size) # [N, seqlen, vocab_size]\n",
    "\n",
    "cells = [rnn.BasicLSTMCell(hidden_size) for _ in range(num_layers)]\n",
    "cells = rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "outputs, states = tf.nn.dynamic_rnn(cells, x_one_hot, \n",
    "                                    initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "outputs = tf.reshape(outputs, [-1, hidden_size])    # [N x seqlen, hidden]\n",
    "logits = layers.fully_connected(outputs, vocab_size,\n",
    "                                activation_fn=None) # [N x seqlen, vocab_size]\n",
    "y_flat = tf.reshape(y_one_hot, [-1, vocab_size])    # [N x seqlen, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_op = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_flat) # [N x seqlen]\n",
    "loss_op = tf.reduce_mean(loss_op)\n",
    "\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_op)\n",
    "\n",
    "y_softmax = tf.nn.softmax(logits)         # [N x seqlen, vocab_size]\n",
    "pred = tf.argmax(y_softmax, axis=1)       # [N x seqlen]\n",
    "pred = tf.reshape(pred, [batch_size, -1]) # [N, seqlen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000500 step, loss:1.8760\n",
      "ett aish aour toanne\n",
      "\tAf tours\n",
      "00001000 step, loss:1.3875\n",
      " tes taocest on \n",
      "\n",
      "Hecond Clown\n",
      "00001500 step, loss:0.8880\n",
      " r t will dse no mrt \n",
      "\tTad yet\n",
      "00002000 step, loss:0.8957\n",
      " tis oetiness. thet he \tsings \n",
      "00002500 step, loss:0.4864\n",
      "tartahet eiolence,\n",
      "\tTo make it\n",
      "00003000 step, loss:0.5192\n",
      "te  ,\tSfd to the manner born, \n",
      "00003500 step, loss:0.4428\n",
      "i  \tTo  see that noble and mos\n",
      "00004000 step, loss:0.4140\n",
      "d with ho addition,\n",
      "\tWe go to \n",
      "00004500 step, loss:0.4181\n",
      " f the ulashing in her galled \n",
      "00005000 step, loss:0.3737\n",
      "\tTnd ti cast away moan:\n",
      "\tGod h\n",
      "00005500 step, loss:0.4234\n",
      "dIhe ss importunate, indeed di\n",
      "00006000 step, loss:0.3670\n",
      "nht bis quietus make\n",
      "\tWith a b\n",
      "00006500 step, loss:0.3475\n",
      " tler shouldst thou be than th\n",
      "00007000 step, loss:0.3613\n",
      "ir toan:\n",
      "\tGod ha' mercy on his\n",
      "00007500 step, loss:0.4041\n",
      " nthet in anger.\n",
      "\n",
      "HAMLET\tPale \n",
      "00008000 step, loss:0.4032\n",
      "u  elto your beauty.\n",
      "\n",
      "OPHELIA\t\n",
      "00008500 step, loss:0.3390\n",
      "  rnsthe winds of heaven\n",
      "\tVisi\n",
      "00009000 step, loss:0.3753\n",
      " aven at him. Your\n",
      "\tworm is yo\n",
      "00009500 step, loss:0.3627\n",
      " \tA taw him yesterday, or t' o\n",
      "00010000 step, loss:0.4112\n",
      "ne tammandment for their death\n",
      "00010500 step, loss:0.3320\n",
      "ahrd's; chapless, and\n",
      "\tknocked\n",
      "00011000 step, loss:0.3433\n",
      " r hwith strings of steel,\n",
      "\tBe\n",
      "00011500 step, loss:0.3569\n",
      "RD POLONIUS\t[ou snow, sometime\n",
      "00012000 step, loss:0.3231\n",
      "eet  thos deed, for thine espe\n",
      "00012500 step, loss:0.3405\n",
      "ths 'as I have seen it in his \n",
      "00013000 step, loss:0.3443\n",
      "    torching]\n",
      "\n",
      "PRINCE FORTINBR\n",
      "00013500 step, loss:0.3674\n",
      " od lord,-\n",
      "\n",
      "HAMLET\tI am very g\n",
      "00014000 step, loss:0.3444\n",
      "Tnterwards, if they should gro\n",
      "00014500 step, loss:0.3523\n",
      " ce bet\n",
      "\tagainst the Danish. W\n",
      "00015000 step, loss:0.3530\n",
      "dtis lather\n",
      "\tLives almost by h\n",
      "00015500 step, loss:0.3416\n",
      "  \tAur chown, our life, and al\n",
      "00016000 step, loss:0.3123\n",
      "\n",
      "\n",
      "\t[ho queen, the courtiers: w\n",
      "00016500 step, loss:0.3375\n",
      "et wheep of death what dreams \n",
      "00017000 step, loss:0.3183\n",
      "t ll hollow thee.\n",
      "\n",
      "\tARCELLUS\tY\n",
      "00017500 step, loss:0.3848\n",
      " athe sunday from the week;\n",
      "\tW\n",
      "00018000 step, loss:0.3789\n",
      "\tTou meve been talk'd of since\n",
      "00018500 step, loss:0.3337\n",
      "ue \tteautified Ophelia,'--\n",
      "\tTh\n",
      "00019000 step, loss:0.3552\n",
      "  ty saughter.--My honourable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, \"checkpoints/char-rnn\")\n",
    "    for step in range(100000):\n",
    "        batch_X, batch_y = loader.next_batch(batch_size, max_length)\n",
    "        loss, _ = sess.run([loss_op, opt], feed_dict={X: batch_X, y: batch_y})\n",
    "        \n",
    "        if (step+1) % 500 == 0:\n",
    "            print(\"{:08d} step, loss:{:.4f}\".format(step+1, loss))\n",
    "            \n",
    "            random = np.random.randint(0, batch_size)\n",
    "            results = sess.run(pred, feed_dict={X: batch_X})\n",
    "            words = [loader.words[word] for word in results[random]] #[200, 30] -> 랜덤 -> [30]\n",
    "            print(\"\".join(words))\n",
    "     \n",
    "        if (step+1) % 5000 == 0: \n",
    "            saver.save(sess, \"checkpoints/char-rnn_\"+str(step+1)) # [char-rnn_20000.ckpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
